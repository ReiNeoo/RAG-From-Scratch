{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents vectorized and saved successfully!\n",
      "Number of requested results 3 is greater than number of elements in index 2, updating n_results = 2\n",
      "Result 1:\n",
      "The Rise of Artificial Intelligence\n",
      "\n",
      "Artificial Intelligence (AI) is revolutionizing various industries by enhancing automation, decision-making, and data analysis. With advancements in deep learning models and natural language processing, AI is now integrated into everyday applications.\n",
      "\n",
      "Key Benefits of AI:\n",
      "1. Improved efficiency in repetitive tasks.\n",
      "2. Enhanced decision-making using data-driven insights.\n",
      "3. Automation of complex processes in industries like healthcare, finance, and aerospace.\n",
      "\n",
      "Result 2:\n",
      "Challenges in AI:\n",
      "- Ethical concerns regarding data privacy.\n",
      "- Potential job displacement due to automation.\n",
      "- Ensuring AI models are fair and unbiased.\n",
      "\n",
      "The future of AI holds promise for personalized services, smarter automation, and enhanced human-computer interactions.\n"
     ]
    }
   ],
   "source": [
    "def read_large_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            yield line.strip()\n",
    "\n",
    "# Efficiently processing lines without loading the entire file into memory\n",
    "for line in read_large_file(\"large_text_file.txt\"):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import GroqAPI\n",
    "import logging\n",
    "\n",
    "class QAChain:\n",
    "    def __init__(self, chroma_path: str, model_name: str = \"groq-api\"):\n",
    "        # Logging setup\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Load embeddings and vectorstore\n",
    "        self.logger.info(\"Loading embeddings and vectorstore...\")\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.vectorstore = Chroma(persist_directory=chroma_path, embedding_function=self.embeddings)\n",
    "\n",
    "        # Load language model\n",
    "        self.logger.info(\"Initializing language model...\")\n",
    "        self.llm = GroqAPI(api_key=\"YOUR_GROQ_API_KEY\")\n",
    "\n",
    "        # Prompt template\n",
    "        self.prompt_template = PromptTemplate(\n",
    "            template=(\n",
    "                \"You are an AI assistant. Answer the following question accurately based on the provided context:\\n\"\n",
    "                \"Context: {context}\\n\"\n",
    "                \"Question: {question}\\n\"\n",
    "                \"Answer:\"\n",
    "            ),\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "\n",
    "        # QA Chain\n",
    "        self.chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            retriever=self.vectorstore.as_retriever(),\n",
    "            chain_type=\"stuff\",  # 'stuff' works best for short, concise answers\n",
    "            chain_type_kwargs={\"prompt\": self.prompt_template}\n",
    "        )\n",
    "\n",
    "    def query(self, question: str) -> str:\n",
    "        \"\"\"Query the QA chain and return the result.\"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Received question: {question}\")\n",
    "            result = self.chain.run(question)\n",
    "            self.logger.info(f\"Generated answer: {result}\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during QAChain query: {e}\")\n",
    "            return \"I'm sorry, but I couldn't retrieve the information you requested.\"\n",
    "\n",
    "    def add_document(self, doc_text: str, doc_id: str):\n",
    "        \"\"\"Add a new document to the vectorstore.\"\"\"\n",
    "        try:\n",
    "            self.vectorstore.add_texts([doc_text], metadatas=[{\"doc_id\": doc_id}])\n",
    "            self.vectorstore.persist()\n",
    "            self.logger.info(f\"Document {doc_id} added successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error adding document: {e}\")\n",
    "\n",
    "    def batch_query(self, questions: list[str]) -> list[str]:\n",
    "        \"\"\"Efficient batch processing for multiple queries.\"\"\"\n",
    "        responses = []\n",
    "        for question in questions:\n",
    "            responses.append(self.query(question))\n",
    "        return responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".hfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
